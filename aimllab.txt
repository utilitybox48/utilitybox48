1. Write a program to demonstrate linear regression using an appropriate dataset.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the California Housing dataset as an alternative
data = fetch_california_housing()
X = data.data[:, 3].reshape(-1, 1)  # Use 'AveRooms' (Feature index 3) as a substitute for RM
y = data.target  # The target is the median house value

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared Score:", r2)

# Step 6: Visualize the results
plt.scatter(X_test, y_test, color='blue', label='Actual Data')
plt.plot(X_test, y_pred, color='red', label='Regression Line')
plt.title('Linear Regression on California Housing Data')
plt.xlabel('Average Number of Rooms (AveRooms)')
plt.ylabel('Median House Value ($100k)')
plt.legend()
plt.show()


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#2. Write a program to logistic regression using an appropriate dataset.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Step 1: Load the Breast Cancer dataset
data = load_breast_cancer()
X = data.data  # Features
y = data.target  # Target (binary classification: malignant or benign)

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train the logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Step 4: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 6: Visualize the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#3. Write a program to demonstrate the working of the decision tree. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier, plot_tree 
from sklearn.metrics import accuracy_score, classification_report 

# Step 1: Load the Iris dataset 
data = load_iris() 
X = data.data  # Features 
y = data.target  # Target (class labels) 

# Step 2: Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# Step 3: Create and train the decision tree model 
model = DecisionTreeClassifier(random_state=42) 
model.fit(X_train, y_train) 

# Step 4: Make predictions on the test set 
y_pred = model.predict(X_test) 

# Step 5: Evaluate the model 
accuracy = accuracy_score(y_test, y_pred) 
print("Accuracy:", accuracy) 
print("Classification Report:\n", classification_report(y_test, y_pred)) 
 
# Step 6: Visualize the decision tree 
plt.figure(figsize=(12, 8)) 
plot_tree(model, feature_names=data.feature_names, class_names=data.target_names, filled=True) 
plt.title('Decision Tree Visualization') 
plt.show() 
 
# Step 7: Classify a new sample 
new_sample = np.array([[5.1, 3.5, 1.4, 0.2]])  # Example input (Sepal Length, Sepal Width, Petal Length, Petal Width) 
prediction = model.predict(new_sample) 
print("New Sample Classification:", data.target_names[prediction[0]]) 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#4. Write a program to implement Naive Bayes algorithm to classify the iris data set. Print both correct and wrong predictions. 

import numpy as np 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import accuracy_score, classification_report 

# Step 1: Load the Iris dataset 
data = load_iris() 
X = data.data  # Features 
y = data.target  # Target (class labels) 

# Step 2: Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# Step 3: Create and train the Naive Bayes model 
model = GaussianNB() 
model.fit(X_train, y_train) 

# Step 4: Make predictions on the test set 
y_pred = model.predict(X_test) 

# Step 5: Print correct and wrong predictions 
correct_predictions = [(X_test[i], y_test[i]) for i in range(len(y_test)) if y_test[i] == y_pred[i]] 
wrong_predictions = [(X_test[i], y_test[i], y_pred[i]) for i in range(len(y_test)) if y_test[i] != y_pred[i]] 

print("Correct Predictions:") 
for sample, label in correct_predictions: 
    print(f"Sample: {sample}, Label: {data.target_names[label]}") 
    
print("\nWrong Predictions:") 
for sample, true_label, pred_label in wrong_predictions: 
    print(f"Sample: {sample}, True Label: {data.target_names[true_label]}, Predicted Label: {data.target_names[pred_label]}") 
    
# Step 6: Evaluate the model 
accuracy = accuracy_score(y_test, y_pred) 
print("\nAccuracy:", accuracy) 
print("\nClassification Report:\n", classification_report(y_test, y_pred, 
target_names=data.target_names))

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#5. Write a program to implement k-Nearest Neighbor algorithm to classify the iris data set. Print both correct and wrong predictions.

from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np

dataset=load_iris()
#print(dataset)
X_train,X_test,y_train,y_test=train_test_split(dataset["data"],dataset["target"],random_state=0)

kn=KNeighborsClassifier(n_neighbors=1)
kn.fit(X_train,y_train)

for i in range(len(X_test)):
    x=X_test[i]
    x_new=np.array([x])
    prediction=kn.predict(x_new)
    print("TARGET=",y_test[i],dataset["target_names"][y_test[i]],"PREDICTED=",prediction,dataset["target_names"][prediction])
print(kn.score(X_test,y_test))
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# 6. Write a program to implement clustering using the k-Means algorithm using an appropriate dataset.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Iris dataset
data = load_iris()
X = data.data  # Features

# Step 2: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Apply the k-Means algorithm
kmeans = KMeans(n_clusters=3, random_state=42)  # Set n_clusters to 3 for Iris dataset
kmeans.fit(X_scaled)

# Step 4: Get the cluster centers and labels
cluster_centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Step 5: Visualize the clustering result (only using the first two features for simplicity)
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=100, alpha=0.6)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], color='red', marker='x', s=200, label='Centroids')
plt.title('k-Means Clustering (Iris Dataset)')
plt.xlabel('Feature 1 (Standardized)')
plt.ylabel('Feature 2 (Standardized)')
plt.legend()
plt.show()

# Step 6: Output the cluster centers and labels
print("Cluster Centers:\n", cluster_centers)
print("\nCluster Labels:", labels)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#7. Write a program to implement Hierarchical clustering 

import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris 
from sklearn.cluster import AgglomerativeClustering 
from sklearn.preprocessing import StandardScaler 
from scipy.cluster.hierarchy import dendrogram, linkage 

# Step 1: Load the Iris dataset 
data = load_iris() 
X = data.data  # Features 

# Step 2: Standardize the features 
scaler = StandardScaler() 
X_scaled = scaler.fit_transform(X) 

# Step 3: Apply Agglomerative Hierarchical Clustering 
agg_clust = AgglomerativeClustering(n_clusters=3, linkage='ward') 
labels = agg_clust.fit_predict(X_scaled) 

# Step 4: Visualize the results using a dendrogram 
linked = linkage(X_scaled, method='ward') 
plt.figure(figsize=(5, 5)) 
dendrogram(linked, labels=data.target_names[labels]) 
plt.title('Hierarchical Clustering Dendrogram (Iris Dataset)') 
plt.xlabel('Sample index') 
plt.ylabel('Distance') 
plt.show() 

# Step 5: Output the cluster labels 
print("Cluster Labels: ", labels) 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#8. Write a program to implement Support Vector Machine algorithm 

import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.svm import SVC 
from sklearn.metrics import accuracy_score, classification_report 

# Step 1: Load the Iris dataset 
data = load_iris() 
X = data.data  # Features 
y = data.target  # Target (class labels) 

# Step 2: Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 

# Step 3: Create and train the SVM model 
model = SVC(kernel='linear')  # Using a linear kernel 
model.fit(X_train, y_train) 

# Step 4: Make predictions on the test set 
y_pred = model.predict(X_test) 

# Step 5: Evaluate the model 
accuracy = accuracy_score(y_test, y_pred) 
class_report = classification_report(y_test, y_pred, target_names=data.target_names) 

# Step 6: Print the evaluation results 
print("Accuracy: ", accuracy) 
print("\nClassification Report:\n", class_report) 

# Step 7: Visualize the decision boundary (Only using the first two features for simplicity) 
plt.figure(figsize=(8, 6)) 
# Plot the data points 
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k', marker='o', s=100, 
alpha=0.7) 
# Plot decision boundary 
h = 0.02  # Step size in the mesh 
x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1 
y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1 
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) 
Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) 
Z = Z.reshape(xx.shape) 
plt.contourf(xx, yy, Z, alpha=0.3) 
plt.xlabel("Feature 1") 
plt.ylabel("Feature 2") 
plt.title("SVM Classifier (Iris Dataset)") 
plt.colorbar() 
plt.show() 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#9. Write a program to implement clustering using random forest algorithm using an appropriate dataset. 

import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score 

# Step 1: Load the Iris dataset 
data = load_iris() 
X = data.data  # Features 
y = data.target  # Target (class labels) 

# Step 2: Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 

# Step 3: Create and train the Random Forest classifier 
rf_model = RandomForestClassifier(n_estimators=100, random_state=42) 
rf_model.fit(X_train, y_train) 

# Step 4: Make predictions on the test set 
y_pred = rf_model.predict(X_test) 

# Step 5: Evaluate the model 
accuracy = accuracy_score(y_test, y_pred) 
print("Accuracy: ", accuracy) 

# Step 6: Visualize the clusters (use the first two features for simplicity) 
plt.figure(figsize=(8, 6)) 
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='viridis', marker='o', edgecolor='k', s=100, 
alpha=0.6) 
plt.title('Random Forest Classifier "Clustering" (Iris Dataset)') 
plt.xlabel('Feature 1') 
plt.ylabel('Feature 2') 
plt.colorbar(label='Predicted Cluster') 
plt.show() 

# Step 7: Output the predicted labels (cluster-like grouping) 
print("Predicted Cluster Labels: ", y_pred)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#10. Write a program to implement Forecasting using ARIMA Model 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from statsmodels.tsa.arima.model import ARIMA 
from sklearn.model_selection import train_test_split 
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf 

# Step 1: Load the dataset (Airline Passengers dataset) 
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv' 
data = pd.read_csv(url, header=0, parse_dates=[0], index_col=0) 

# Step 2: Visualize the dataset 
plt.figure(figsize=(10,6)) 
plt.plot(data) 
plt.title('Monthly Number of Airline Passengers') 
plt.xlabel('Date') 
plt.ylabel('Number of Passengers') 
plt.show() 

# Step 3: Split the data into training and testing sets 
train_size = int(len(data) * 0.8) 
train, test = data[0:train_size], data[train_size:] 

# Step 4: Visualize ACF and PACF plots for choosing p, d, q values 
plot_acf(train, lags=40) 
plot_pacf(train, lags=40) 
plt.show() 

# Step 5: Fit the ARIMA model 
model = ARIMA(train, order=(5, 1, 0)) 
model_fit = model.fit() 

# Step 6: Make predictions on the test set 
forecast = model_fit.forecast(steps=len(test)) 

# Step 7: Visualize the actual vs predicted values 
plt.figure(figsize=(10,6)) 
plt.plot(train, label='Train') 
plt.plot(test, label='Test') 
plt.plot(test.index, forecast, label='Forecast', color='red') 
plt.title('ARIMA Model Forecasting') 
plt.xlabel('Date') 
plt.ylabel('Number of Passengers') 
plt.legend() 
plt.show() 

# Step 8: Evaluate the model 
from sklearn.metrics import mean_squared_error 
mse = mean_squared_error(test, forecast) 
print(f'Mean Squared Error: {mse}') 
